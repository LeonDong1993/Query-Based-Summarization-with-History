# Query Based Summarization with History

## Introduction
This is a course project for CS6301-Deep Learning in NLP (Spring 2023) at UT Dallas. In this project, we aim to boost the query-based summarization performance by utilizing the most related query and its summary from query histories. We fine-tuned BART model and conducted several experiments on a meeting summarization task.

## Method
Standard query based summarization can be solved by encoding the query Q and source text T as `<s> Q </s> T </s>`, then any sequence to sequence model such as BART can be used to generate the summary/answer.

We want to exploit the possibility of using query histories to boosting the query-based summarization performance. Specifically, for current query `Q` and source text `T`, we find another query `Qr` from the query history with its corresponding answer/summary `Qs` and then put all these four parts into model as `<s> Qr </s> Qs </s> Q </s> T </s>`.

The intuition behind this idea is that, just like doing hard math problem that have multiple sub questions. Knowing the sub question (1) and (2) along with its answer should be able to better help you work on a more hard ones such as the last question (3).


## Results
The initial results is shown in the following table.
- The BART model we trained achieved significantly better results than that reported in the original QMSum paper.
- Our extra input does not improve the query performance.

![initial results](https://github.com/LeonDong1993/Query-Based-Summarization-with-History/blob/main/figures/initial.png)

We analyzed the potential reasons and conducted another experiment (details can be checked in our slides in `Other resources` section).
- Now we can achieve similarly performance.
- if we feed golden related summary at test time, we achieve the best performance.

![final results](https://github.com/LeonDong1993/Query-Based-Summarization-with-History/blob/main/figures/final.png)


## Install
There is some dependencies you need to install before you can execute our notebooks, such as `Pytorch`, `transformers` and `datasets`.


## Usage
This repo mainly consists of three notebooks.

`baseline.ipynb`: we fine-tuned a BART model with standard input `<s> Q </s> T </s>` and evaluated on QMsum dataset. It contains the snapshot of initial experiment results.

`ours.ipynb`: we fine-tuned another BART model with our input  `<s> Qr </s> Qs </s> Q </s> T </s>` and evaluated on QMsum dataset as well. During test time, the `Qs` is generated by model itself on the fly. This notebook is just simple modification over the `baseline.ipynb`.  It contains the snapshot of initial experiment results.

`experiments.ipynb`: A combination of above two (remove the redundant codes, and add some extra features), this notebook contains the snapshot the final experiments results (after we analysis the results and made some modifications to the hyper-parameters and datasets). **(This notebook is what you needed only)**


## Other Resources
The slides of this project can be found at this [Google Doc](https://docs.google.com/presentation/d/1YnejWzuYDuVe8avwwbZQnAy7W2LeQH8-y9bL593CAOY/edit?usp=sharing).

The report of this project will be public available as well (will be directly add into this repo).

## References
1. [QMSum Dataset](https://github.com/Yale-LILY/QMSum)
2. [BART Model](https://arxiv.org/abs/1910.13461)



