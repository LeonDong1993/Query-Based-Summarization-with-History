{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25fce4b",
   "metadata": {},
   "source": [
    "# You need to download the dataset first \n",
    "! git clone https://github.com/Yale-LILY/QMSum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ce66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pdb\n",
    "import numpy as np \n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# define some global variables\n",
    "# not a good style, but very convenient in notebook experiment\n",
    "BERT_MODEL = None\n",
    "NN_DEVICE = 'cuda:0'\n",
    "data_root = 'QMSum/data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb4f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    tokens = ' '.join(word_tokenize(sent.lower()))\n",
    "    return tokens\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.replace('{ vocalsound } ', '')\n",
    "    text = text.replace('{ disfmarker } ', '')\n",
    "    text = text.replace('a_m_i_', 'ami')\n",
    "    text = text.replace('l_c_d_', 'lcd')\n",
    "    text = text.replace('p_m_s', 'pms')\n",
    "    text = text.replace('t_v_', 'tv')\n",
    "    text = text.replace('{ pause } ', '')\n",
    "    text = text.replace('{ nonvocalsound } ', '')\n",
    "    text = text.replace('{ gap } ', '')\n",
    "    return text\n",
    "\n",
    "def encode(*args):\n",
    "    return '<s> {} </s>'.format( ' </s> '.join(args))\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    global BERT_MODEL\n",
    "    \n",
    "    # initialize the model if first time run\n",
    "    if BERT_MODEL is None:\n",
    "        model_config = 'bert-base-uncased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_config)\n",
    "        model = BertModel.from_pretrained(model_config, output_hidden_states=True)\n",
    "        model.to(NN_DEVICE)\n",
    "        model.eval()\n",
    "        BERT_MODEL = (tokenizer, model)\n",
    "        \n",
    "    tokenizer, model = BERT_MODEL\n",
    "    ids = tokenizer.encode(text)\n",
    "    ids = torch.IntTensor(ids).unsqueeze(0).to(NN_DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids = ids)\n",
    "    \n",
    "    # extract the last 4 layer hidden state\n",
    "    # and use that to form a sentence embedding\n",
    "    hidden_states = output[2]\n",
    "    features = torch.cat([hidden_states[-i] for i in range(4)] , dim=-1)\n",
    "    features = features.squeeze().cpu().numpy()\n",
    "    text_embedding = np.mean(features, axis = 0)\n",
    "    return text_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b316f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "# similarity function for bert sentence embedding\n",
    "def cosine_sim(x, y):\n",
    "    nx = np.linalg.norm(x)\n",
    "    ny = np.linalg.norm(y)\n",
    "    return np.sum(x*y) / (nx * ny)\n",
    "\n",
    "# similarity function for text span \n",
    "def iou_sim(x,y):\n",
    "    a,b = x\n",
    "    c,d = y\n",
    "    union = max(b,d) - min(a,c)\n",
    "    intersection = min(b,d) - max(a,c)\n",
    "    intersection = max(0, intersection)\n",
    "    return intersection / union\n",
    "  \n",
    "def get_related_query(query_feature, sim_func, train = False):\n",
    "    '''\n",
    "    args\n",
    "    ---------\n",
    "    query_feature - a list of bert_embedding/relavant_text_span of each query\n",
    "    sim_func - a function used to compute the similarity between query features\n",
    "    train - indicated whether is called for train data\n",
    "    \n",
    "    returns \n",
    "    ---------\n",
    "    1. the order to evaluate the queries\n",
    "    2. the related query of current query\n",
    "    \n",
    "    side note \n",
    "    ------------\n",
    "    In practice, because we don't know all the question in advance, the order is \n",
    "    fixed, and what we can do is to find most relavant history queries.\n",
    "    '''\n",
    "    n = len(query_feature)\n",
    "    similarity = np.zeros(shape = (n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            similarity[i,j] = sim_func(query_feature[i], query_feature[j])\n",
    "            similarity[j,i] = similarity[i,j]\n",
    "    \n",
    "    # use a heuristic algorithm\n",
    "    weights = -np.sum(similarity, axis = 0)\n",
    "    order =  np.argsort(weights)\n",
    "    related = [0] * n\n",
    "    \n",
    "    for i, k in enumerate(order):\n",
    "        if not train and i == 0:\n",
    "            related[k] = None\n",
    "            continue \n",
    "        \n",
    "        if train:\n",
    "            selected = order\n",
    "        else:\n",
    "            selected = order[0:i]\n",
    "        \n",
    "        j = np.argmax(similarity[k][selected])\n",
    "        \n",
    "        if k == selected[j]:\n",
    "            related[k] = None\n",
    "        else:\n",
    "            related[k] = selected[j]\n",
    "    \n",
    "    \n",
    "    return order, related\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bd32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root_dir, split, metric = None):\n",
    "    '''\n",
    "    metric - can only be bert/none\n",
    "    '''\n",
    "    \n",
    "    json_data_path = f'{root_dir}/ALL/jsonl/{split}.jsonl'\n",
    "    with open(json_data_path) as f:\n",
    "        meetings = [json.loads(line) for line in f]\n",
    "        \n",
    "    print('Loaded {} meetings in {} set'.format(len(meetings), split))\n",
    "    \n",
    "    data = [] \n",
    "    eval_orders = []\n",
    "    \n",
    "    for cur_meet in meetings:\n",
    "        turns = []\n",
    "        for item in cur_meet['meeting_transcripts']:\n",
    "            turns.append('{}: {}'.format(item['speaker'].lower(), tokenize(item['content']) ))\n",
    "        entire_src = ' '.join(turns)\n",
    "        \n",
    "        for key_name in ['general_query_list', 'specific_query_list']:\n",
    "            queries = [tokenize(item['query']) for item in cur_meet[key_name]]\n",
    "            answers = [tokenize(item['answer']) for item in cur_meet[key_name]]\n",
    "            \n",
    "            offset = len(data)\n",
    "            if metric is not None:\n",
    "                query_feature = [get_bert_embedding(q) for q in queries] \n",
    "                order, related = get_related_query(query_feature, cosine_sim, split == 'train')\n",
    "                order = np.array(order) + offset\n",
    "            else:\n",
    "                order, related = None, None\n",
    "            eval_orders.append(order)\n",
    "\n",
    "            for i, item in enumerate(cur_meet[key_name]):\n",
    "                cur = dict()\n",
    "                cur['tgt'] = answers[i]\n",
    "                cur['query'] = queries[i]\n",
    "                if 'general' in key_name:\n",
    "                    text = entire_src\n",
    "                else:\n",
    "                    selected_turns = []\n",
    "                    for st, ed in item['relevant_text_span']:\n",
    "                        st, ed = int(st), int(ed)\n",
    "                        for k in range(st, ed+1):\n",
    "                            selected_turns.append( turns[k] )\n",
    "                    text = ' '.join(selected_turns)\n",
    "                cur['text'] = text\n",
    "                \n",
    "                #  ----------------------  #\n",
    "                query = cur['query']\n",
    "                if metric is not None:\n",
    "                    j = related[i]\n",
    "\n",
    "                    if j is None:\n",
    "                        rq, ra = ' ', ' '\n",
    "                        cur['rq_index'] = None\n",
    "                    else:\n",
    "                        rq, ra = queries[j], answers[j]\n",
    "                        cur['rq_index'] = j + offset\n",
    "                    \n",
    "                    if split == 'train':\n",
    "                        cur['src'] = clean_data(encode(rq, ra, query, text))\n",
    "                        \n",
    "                else:\n",
    "                    cur['src'] = clean_data(encode(query, text))\n",
    "                data.append(cur)\n",
    "    \n",
    "    if metric is not None:\n",
    "        eval_orders = list(np.concatenate(eval_orders) )\n",
    "    \n",
    "    return data, eval_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf11615",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 162 meetings in train set\n",
      "Loaded 35 meetings in val set\n",
      "Loaded 35 meetings in test set\n"
     ]
    }
   ],
   "source": [
    "# load all the required data\n",
    "# we can load the original format in QMSum paper if we set load_metric to None\n",
    "load_metric = None\n",
    "train_data, _ = load_data(data_root, 'train', load_metric)\n",
    "val_data, val_order = load_data(data_root, 'val', load_metric)\n",
    "test_data, test_order = load_data(data_root, 'test', load_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d05a988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 1257\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 281\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the training dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict()\n",
    "for name, data in [('train', train_data), ('val',val_data), ('test', test_data)]:\n",
    "    d = {'src': [item['src'] for item in data], 'tgt':[item['tgt'] for item in data]}\n",
    "    df = pd.DataFrame(data = d)    \n",
    "    dataset[name] = Dataset.from_pandas(df, split = name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c954a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1257\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 281\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tokenizer and process the data\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "MAX_TOKENS = 2048      # same as the QMSum paper\n",
    "\n",
    "def process(instance, **kwargs):\n",
    "    return tokenizer(instance['src'], text_target=instance['tgt'], max_length = MAX_TOKENS, truncation=True, **kwargs)\n",
    "\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenized_data = dataset.map(process, batched = True, remove_columns=['src', 'tgt'])\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b23bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and modify the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# change model architecture a little bit to support 2048 tokens \n",
    "if MAX_TOKENS == 2048:\n",
    "    sd = model.state_dict()\n",
    "    ori_pe = sd['model.encoder.embed_positions.weight']\n",
    "    new_pe = torch.cat([ori_pe[:-1], ori_pe[1:]], axis = 0)\n",
    "    new_pe.requires_grad = True\n",
    "    sd['model.decoder.embed_positions.weight'] = new_pe\n",
    "    sd['model.encoder.embed_positions.weight'] = new_pe\n",
    "\n",
    "    new_config = model.config\n",
    "    new_config.max_position_embeddings = MAX_TOKENS\n",
    "    new_model = AutoModelForSeq2SeqLM.from_config(new_config)\n",
    "    new_model.load_state_dict(sd, strict=True)\n",
    "    model = new_model\n",
    "    \n",
    "model.to(NN_DEVICE)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7740f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation related block of codes\n",
    "import nltk\n",
    "import evaluate\n",
    "ROUGE = evaluate.load(\"rouge\")\n",
    "\n",
    "MAX_GENERATION = 160\n",
    "\n",
    "# this function is from internet for rouge evaluation \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = ROUGE.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def evaluate(model, data, order):\n",
    "    n = len(data)\n",
    "    summaries = [None] * n\n",
    "    preds = []\n",
    "    label = []\n",
    "    for i in order:\n",
    "        item = data[i]\n",
    "        j = item['rq_index']\n",
    "        if j is None:\n",
    "            rq, ra = ' ', ' '\n",
    "        else:\n",
    "            rq = data[j]['query']\n",
    "            ra = summaries[j]\n",
    "            assert(ra is not None), \"Error with evaluation order\"\n",
    "        \n",
    "        item['src'] = clean_data(encode(rq, ra, item['query'], item['text']))\n",
    "        inputs = process(item, return_tensors = 'pt')\n",
    "        tensor_input = {k: inputs[k].to(NN_DEVICE) for k in inputs}\n",
    "        output = model.generate(**tensor_input, num_beams = 4, min_length = 30, max_length = MAX_GENERATION, do_sample = True)\n",
    "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        summaries[i] = summary\n",
    "        preds.append(output[0].cpu().numpy())\n",
    "        label.append(inputs['labels'][0].cpu().numpy())\n",
    "        \n",
    "    max_pred_len = max(len(item) for item in preds)\n",
    "    max_label_len =max(len(item) for item in label)\n",
    "    pred_array =  np.zeros(shape = (len(preds), max_pred_len), dtype = 'i4' ) + 1\n",
    "    label_array = np.zeros(shape = (len(label), max_label_len), dtype = 'i4' ) - 100\n",
    "    for i, item in enumerate(preds):\n",
    "        n = item.size\n",
    "        pred_array[i][:n] = item\n",
    "    \n",
    "    for i, item in enumerate(label):\n",
    "        n = item.size\n",
    "        label_array[i][:n] = item\n",
    "    \n",
    "    print( compute_metrics( (pred_array, label_array) ) )\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7dd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to special evaluation order of our dataset \n",
    "# we need to use callback to evaluate the performance on validation dataset\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class EvalCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        evaluate(model, val_data, val_order)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7468136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fine tuning \n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = 'steps',\n",
    "    learning_rate = 1e-5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    logging_steps = 4000,\n",
    "    weight_decay = 1e-4,\n",
    "    save_total_limit = 3,\n",
    "    num_train_epochs = 80,\n",
    "    predict_with_generate = True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'],\n",
    "    eval_dataset = tokenized_data['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18facf7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lab/HailiangDong/projs/pyenv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3f5347e9734a95a60314d4854224e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50320' max='50320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50320/50320 05:00, Epoch 80/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50320, training_loss=6.178790771525312e-06, metrics={'train_runtime': 303.0843, 'train_samples_per_second': 331.789, 'train_steps_per_second': 166.026, 'total_flos': 3.0681428033367245e+17, 'train_loss': 6.178790771525312e-06, 'epoch': 80.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ffeba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='277' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 09:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.210109710693359,\n",
       " 'eval_rouge1': 39.5366,\n",
       " 'eval_rouge2': 13.2804,\n",
       " 'eval_rougeL': 25.1704,\n",
       " 'eval_rougeLsum': 34.6919,\n",
       " 'eval_gen_len': 85.7279,\n",
       " 'eval_runtime': 293.4395,\n",
       " 'eval_samples_per_second': 0.927,\n",
       " 'eval_steps_per_second': 0.463,\n",
       " 'epoch': 80.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a final evaluation on validation set\n",
    "trainer.evaluate(tokenized_data['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf92f1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.999124526977539,\n",
       " 'eval_rouge1': 40.1961,\n",
       " 'eval_rouge2': 14.6027,\n",
       " 'eval_rougeL': 26.1468,\n",
       " 'eval_rougeLsum': 35.7441,\n",
       " 'eval_gen_len': 86.8327,\n",
       " 'eval_runtime': 307.1277,\n",
       " 'eval_samples_per_second': 0.915,\n",
       " 'eval_steps_per_second': 0.459,\n",
       " 'epoch': 80.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cace818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99a25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd255475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
