{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25fce4b",
   "metadata": {},
   "source": [
    "# You need to download the dataset first \n",
    "! git clone https://github.com/Yale-LILY/QMSum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ce66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pdb\n",
    "import numpy as np \n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from nltk import word_tokenize\n",
    "from copy import deepcopy\n",
    "\n",
    "# define some global variables\n",
    "# not a good style, but very convenient in notebook experiment\n",
    "BERT_MODEL = None\n",
    "NN_DEVICE = 'cuda:0'\n",
    "data_root = 'QMSum/data/'\n",
    "testing = False\n",
    "with_general_query = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb4f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    tokens = ' '.join(word_tokenize(sent.lower()))\n",
    "    return tokens\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.replace('{ vocalsound } ', '')\n",
    "    text = text.replace('{ disfmarker } ', '')\n",
    "    text = text.replace('a_m_i_', 'ami')\n",
    "    text = text.replace('l_c_d_', 'lcd')\n",
    "    text = text.replace('p_m_s', 'pms')\n",
    "    text = text.replace('t_v_', 'tv')\n",
    "    text = text.replace('{ pause } ', '')\n",
    "    text = text.replace('{ nonvocalsound } ', '')\n",
    "    text = text.replace('{ gap } ', '')\n",
    "    return text\n",
    "\n",
    "def encode(*args):\n",
    "    if len(args) == 2:\n",
    "        return '<s> {} </s> {} </s>'.format(*args)\n",
    "    \n",
    "    elif len(args) == 4:\n",
    "        q,s,rq,ra = args\n",
    "        return '<s> {} {} </s> {} </s> {} </s>'.format(rq,ra,q,s)\n",
    "    \n",
    "    assert(0), 'Wrong input'\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    global BERT_MODEL\n",
    "    \n",
    "    # initialize the model if first time run\n",
    "    if BERT_MODEL is None:\n",
    "        model_config = 'bert-base-uncased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_config)\n",
    "        model = BertModel.from_pretrained(model_config, output_hidden_states=True)\n",
    "        model.to(NN_DEVICE)\n",
    "        model.eval()\n",
    "        BERT_MODEL = (tokenizer, model)\n",
    "        \n",
    "    tokenizer, model = BERT_MODEL\n",
    "    ids = tokenizer.encode(text)\n",
    "    ids = torch.IntTensor(ids).unsqueeze(0).to(NN_DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids = ids)\n",
    "    \n",
    "    # extract the last 4 layer hidden state\n",
    "    # and use that to form a sentence embedding\n",
    "    hidden_states = output[2]\n",
    "    features = torch.cat([hidden_states[-i] for i in range(4)] , dim=-1)\n",
    "    features = features.squeeze().cpu().numpy()\n",
    "    text_embedding = np.mean(features, axis = 0)\n",
    "    return text_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b316f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "# similarity function for bert sentence embedding\n",
    "def cosine_sim(x, y):\n",
    "    nx = np.linalg.norm(x)\n",
    "    ny = np.linalg.norm(y)\n",
    "    return np.sum(x*y) / (nx * ny)\n",
    "\n",
    "# similarity function for text span \n",
    "def iou_sim(x,y):\n",
    "    a,b = x\n",
    "    c,d = y\n",
    "    union = max(b,d) - min(a,c)\n",
    "    intersection = min(b,d) - max(a,c)\n",
    "    intersection = max(0, intersection)\n",
    "    return intersection / union\n",
    "  \n",
    "def get_related_query(query_feature, sim_func, train = False):\n",
    "    '''\n",
    "    args\n",
    "    ---------\n",
    "    query_feature - a list of bert_embedding/relavant_text_span of each query\n",
    "    sim_func - a function used to compute the similarity between query features\n",
    "    train - indicated whether is called for train data\n",
    "    \n",
    "    returns \n",
    "    ---------\n",
    "    1. the order to evaluate the queries\n",
    "    2. the related query of current query\n",
    "    \n",
    "    side note \n",
    "    ------------\n",
    "    In practice, because we don't know all the question in advance, the order is \n",
    "    fixed, and what we can do is to find most relavant history queries.\n",
    "    '''\n",
    "    n = len(query_feature)\n",
    "    similarity = np.zeros(shape = (n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            similarity[i,j] = sim_func(query_feature[i], query_feature[j])\n",
    "            similarity[j,i] = similarity[i,j]\n",
    "    \n",
    "    # use a heuristic algorithm\n",
    "    weights = -np.sum(similarity, axis = 0)\n",
    "    order =  np.argsort(weights)\n",
    "    related = [0] * n\n",
    "    \n",
    "    for i, k in enumerate(order):\n",
    "        if not train and i == 0:\n",
    "            related[k] = None\n",
    "            continue \n",
    "        \n",
    "        if train:\n",
    "            selected = order\n",
    "        else:\n",
    "            selected = order[0:i]\n",
    "        \n",
    "        j = np.argmax(similarity[k][selected])\n",
    "        \n",
    "        if k == selected[j]:\n",
    "            related[k] = None\n",
    "        else:\n",
    "            related[k] = selected[j]\n",
    "    \n",
    "    \n",
    "    return order, related\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bd32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root_dir, split, metric = None):\n",
    "    '''\n",
    "    metric - can only be bert/none\n",
    "    '''\n",
    "    \n",
    "    json_data_path = f'{root_dir}/ALL/jsonl/{split}.jsonl'\n",
    "    with open(json_data_path) as f:\n",
    "        meetings = [json.loads(line) for line in f]\n",
    "        \n",
    "    print('Loaded {} meetings in {} set'.format(len(meetings), split))\n",
    "    if testing:\n",
    "        meetings = meetings[:3]\n",
    "    \n",
    "    data = [] \n",
    "    eval_orders = []\n",
    "    \n",
    "    for cur_meet in meetings:\n",
    "        turns = []\n",
    "        for item in cur_meet['meeting_transcripts']:\n",
    "            turns.append('{}: {}'.format(item['speaker'].lower(), tokenize(item['content']) ))\n",
    "        entire_src = ' '.join(turns)\n",
    "        \n",
    "        query_keys = ['specific_query_list']\n",
    "        if with_general_query:\n",
    "            query_keys.append('general_query_list')\n",
    "        \n",
    "        for key_name in query_keys:\n",
    "            queries = [tokenize(item['query']) for item in cur_meet[key_name]]\n",
    "            answers = [tokenize(item['answer']) for item in cur_meet[key_name]]\n",
    "            \n",
    "            offset = len(data)\n",
    "            if metric is not None:\n",
    "                query_feature = [get_bert_embedding(q) for q in queries] \n",
    "                order, related = get_related_query(query_feature, cosine_sim, split == 'train')\n",
    "                order = np.array(order) + offset\n",
    "            else:\n",
    "                order, related = None, None\n",
    "            eval_orders.append(order)\n",
    "\n",
    "            for i, item in enumerate(cur_meet[key_name]):\n",
    "                cur = dict()\n",
    "                cur['tgt'] = answers[i]\n",
    "                cur['query'] = queries[i]\n",
    "                if 'general' in key_name:\n",
    "                    text = entire_src\n",
    "                else:\n",
    "                    selected_turns = []\n",
    "                    for st, ed in item['relevant_text_span']:\n",
    "                        st, ed = int(st), int(ed)\n",
    "                        for k in range(st, ed+1):\n",
    "                            selected_turns.append( turns[k] )\n",
    "                    text = ' '.join(selected_turns)\n",
    "                cur['text'] = text\n",
    "                \n",
    "                #  ----------------------  #\n",
    "                query = cur['query']\n",
    "                if metric is not None:\n",
    "                    j = related[i]\n",
    "\n",
    "                    if j is None:\n",
    "                        rq, ra = ' ', ' '\n",
    "                        cur['rq_index'] = None\n",
    "                    else:\n",
    "                        rq, ra = queries[j], answers[j]\n",
    "                        cur['rq_index'] = j + offset\n",
    "                    \n",
    "                    if split == 'train':\n",
    "                        cur['src'] = clean_data(encode(query, text, rq,ra))\n",
    "                        \n",
    "                else:\n",
    "                    cur['src'] = clean_data(encode(query, text))\n",
    "                data.append(cur)\n",
    "    \n",
    "    if metric is not None:\n",
    "        eval_orders = list(np.concatenate(eval_orders) )\n",
    "    else:\n",
    "        eval_orders = None\n",
    "    \n",
    "    print('Total number of instances in {} set is {}'.format(split, len(data)))\n",
    "    return data, eval_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead0600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation related block of codes\n",
    "import nltk\n",
    "import evaluate\n",
    "ROUGE = evaluate.load(\"rouge\")\n",
    "\n",
    "MAX_GENERATION = 150\n",
    "\n",
    "# this function is from internet for rouge evaluation \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = ROUGE.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "def evaluate(model, data, order):\n",
    "    n = len(data)\n",
    "    summaries = [None] * n\n",
    "    preds = []\n",
    "    label = []\n",
    "    \n",
    "    if order is None:\n",
    "        order = list(range(n))\n",
    "    \n",
    "    for i in order:\n",
    "        item = data[i]\n",
    "        \n",
    "        if 'rq_index' in item:\n",
    "            j = item['rq_index']\n",
    "            if j is None:\n",
    "                rq, ra = ' ', ' '\n",
    "            else:\n",
    "                rq = data[j]['query']\n",
    "                ra = summaries[j]\n",
    "                assert(ra is not None), \"Error with evaluation order\"\n",
    "\n",
    "            item['src'] = clean_data(encode(item['query'], item['text'], rq, ra))\n",
    "        \n",
    "        inputs = process(item, return_tensors = 'pt')\n",
    "        tensor_input = {k: inputs[k].to(NN_DEVICE) for k in inputs}\n",
    "        output = model.generate(**tensor_input, num_beams = 4, min_length = 30, max_length = MAX_GENERATION, do_sample = True)\n",
    "        # output = model.generate(**tensor_input)\n",
    "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        summaries[i] = summary\n",
    "        preds.append(output[0].cpu().numpy())\n",
    "        label.append(inputs['labels'][0].cpu().numpy())\n",
    "        \n",
    "    max_pred_len = max(len(item) for item in preds)\n",
    "    max_label_len =max(len(item) for item in label)\n",
    "    pred_array =  np.zeros(shape = (len(preds), max_pred_len), dtype = 'i4' ) + 1\n",
    "    label_array = np.zeros(shape = (len(label), max_label_len), dtype = 'i4' ) - 100\n",
    "    for i, item in enumerate(preds):\n",
    "        n = item.size\n",
    "        pred_array[i][:n] = item\n",
    "    \n",
    "    for i, item in enumerate(label):\n",
    "        n = item.size\n",
    "        label_array[i][:n] = item\n",
    "    \n",
    "    print( compute_metrics( (pred_array, label_array) ) )\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7810db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to special evaluation order of our dataset \n",
    "# we need to use callback to evaluate the performance on validation dataset\n",
    "from transformers import TrainerCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "class EvalCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        evaluate(model, val_data, val_order)\n",
    "\n",
    "\n",
    "def get_train_args(out_dir, es):\n",
    "    epochs = 1 if testing else 40\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir = out_dir,\n",
    "        evaluation_strategy = es,\n",
    "        learning_rate = 2e-5,\n",
    "        per_device_train_batch_size = 3,\n",
    "        per_device_eval_batch_size = 3,\n",
    "        logging_steps = 1000,\n",
    "        weight_decay = 5e-3,\n",
    "        save_total_limit = 3,\n",
    "        warmup_steps = 50,\n",
    "        num_train_epochs = epochs,\n",
    "        predict_with_generate = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf11615",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 2048      # same as the QMSum paper\n",
    "\n",
    "def process(instance, **kwargs):\n",
    "    return tokenizer(instance['src'], text_target=instance['tgt'], max_length = MAX_TOKENS, truncation=True, **kwargs)\n",
    "\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# load and modify the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# change model architecture a little bit to support 2048 tokens \n",
    "if MAX_TOKENS == 2048:\n",
    "    sd = model.state_dict()\n",
    "    ori_pe = sd['model.encoder.embed_positions.weight']\n",
    "    new_pe = torch.cat([ori_pe[:-1], ori_pe[1:]], axis = 0)\n",
    "    new_pe.requires_grad = True\n",
    "    sd['model.decoder.embed_positions.weight'] = new_pe\n",
    "    sd['model.encoder.embed_positions.weight'] = new_pe\n",
    "\n",
    "    new_config = model.config\n",
    "    new_config.max_position_embeddings = MAX_TOKENS\n",
    "    new_model = AutoModelForSeq2SeqLM.from_config(new_config)\n",
    "    new_model.load_state_dict(sd, strict=True)\n",
    "else:\n",
    "    new_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c954a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 162 meetings in train set\n",
      "Total number of instances in train set is 1095\n",
      "Loaded 35 meetings in val set\n",
      "Total number of instances in val set is 237\n",
      "Loaded 35 meetings in test set\n",
      "Total number of instances in test set is 244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 1. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f27b60eb8046a5b3fbb2c8b0f7dad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3680' max='3680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3680/3680 24:17, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# standard method \n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "model = deepcopy(new_model)\n",
    "model.to(NN_DEVICE)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding='longest')\n",
    "\n",
    "load_metric = None\n",
    "train_data, _ = load_data(data_root, 'train', load_metric)\n",
    "val_data, val_order = load_data(data_root, 'val', load_metric)\n",
    "test_data, test_order = load_data(data_root, 'test', load_metric)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "for name, data in [('train', train_data), ('val',val_data), ('test', test_data)]:\n",
    "    d = {'src': [item['src'] for item in data], 'tgt':[item['tgt'] for item in data]}\n",
    "    df = pd.DataFrame(data = d)    \n",
    "    dataset[name] = Dataset.from_pandas(df, split = name)\n",
    "\n",
    "tokenized_data = dataset.map(process, batched = True, remove_columns=['src', 'tgt'])\n",
    "\n",
    "# start fine tuning \n",
    "training_args = get_train_args('baseline', 'steps')\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'],\n",
    "    eval_dataset = tokenized_data['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "except:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98df3b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 39.2278, 'rouge2': 14.4858, 'rougeL': 26.1332, 'rougeLsum': 34.5727, 'gen_len': 74.6393}\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_data, val_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038c1aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 39.4057, 'rouge2': 14.4307, 'rougeL': 25.7, 'rougeLsum': 34.3029, 'gen_len': 74.4754}\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_data, test_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd255475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 162 meetings in train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances in train set is 1095\n",
      "Loaded 35 meetings in val set\n",
      "Total number of instances in val set is 237\n",
      "Loaded 35 meetings in test set\n",
      "Total number of instances in test set is 244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 1. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7106d8debf42e99c246f33df8aa923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3680' max='3680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3680/3680 27:24, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailiang/miniconda3/envs/pylab/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 37.7923, 'rouge2': 12.4692, 'rougeL': 24.3542, 'rougeLsum': 32.9965, 'gen_len': 68.7215}\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = deepcopy(new_model)\n",
    "model.to(NN_DEVICE)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding='longest')\n",
    "\n",
    "# our method with extra reference input\n",
    "load_metric = 'bert'\n",
    "train_data, _ = load_data(data_root, 'train', load_metric)\n",
    "val_data, val_order = load_data(data_root, 'val', load_metric)\n",
    "test_data, test_order = load_data(data_root, 'test', load_metric)\n",
    "\n",
    "d = {'src': [item['src'] for item in train_data], 'tgt':[item['tgt'] for item in train_data]}\n",
    "df = pd.DataFrame(data = d)\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = Dataset.from_pandas(df, split = 'train')\n",
    "\n",
    "tokenized_data = dataset.map(process, batched = True, remove_columns=['src', 'tgt'])\n",
    "\n",
    "training_args = get_train_args('ours','no')\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    callbacks = [EvalCallback]\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "except:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f06abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 38.6657, 'rouge2': 13.9371, 'rougeL': 25.4079, 'rougeLsum': 33.9261, 'gen_len': 67.2008}\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_data, val_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b9bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 39.2438, 'rouge2': 14.727, 'rougeL': 26.3423, 'rougeLsum': 34.2354, 'gen_len': 70.25}\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_data, test_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327061b",
   "metadata": {},
   "source": [
    "In the next, we try to see the performance if we pass ground truth summary in test case.\n",
    "\n",
    "we only need to change one line in the evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03c1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, order):\n",
    "    n = len(data)\n",
    "    summaries = [None] * n\n",
    "    preds = []\n",
    "    label = []\n",
    "    \n",
    "    if order is None:\n",
    "        order = list(range(n))\n",
    "    \n",
    "    for i in order:\n",
    "        item = data[i]\n",
    "        \n",
    "        if 'rq_index' in item:\n",
    "            j = item['rq_index']\n",
    "            if j is None:\n",
    "                rq, ra = ' ', ' '\n",
    "            else:\n",
    "                rq = data[j]['query']\n",
    "                # using the ground truth, only this line is changed\n",
    "                ra = data[j]['tgt']\n",
    "                assert(ra is not None), \"Error with evaluation order\"\n",
    "\n",
    "            item['src'] = clean_data(encode(item['query'], item['text'], rq, ra))\n",
    "        \n",
    "        inputs = process(item, return_tensors = 'pt')\n",
    "        tensor_input = {k: inputs[k].to(NN_DEVICE) for k in inputs}\n",
    "        output = model.generate(**tensor_input, num_beams = 4, min_length = 30, max_length = MAX_GENERATION, do_sample = True)\n",
    "        # output = model.generate(**tensor_input)\n",
    "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        summaries[i] = summary\n",
    "        preds.append(output[0].cpu().numpy())\n",
    "        label.append(inputs['labels'][0].cpu().numpy())\n",
    "        \n",
    "    max_pred_len = max(len(item) for item in preds)\n",
    "    max_label_len =max(len(item) for item in label)\n",
    "    pred_array =  np.zeros(shape = (len(preds), max_pred_len), dtype = 'i4' ) + 1\n",
    "    label_array = np.zeros(shape = (len(label), max_label_len), dtype = 'i4' ) - 100\n",
    "    for i, item in enumerate(preds):\n",
    "        n = item.size\n",
    "        pred_array[i][:n] = item\n",
    "    \n",
    "    for i, item in enumerate(label):\n",
    "        n = item.size\n",
    "        label_array[i][:n] = item\n",
    "    \n",
    "    print( compute_metrics( (pred_array, label_array) ) )\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864d9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 40.1789, 'rouge2': 15.2989, 'rougeL': 26.9512, 'rougeLsum': 35.4389, 'gen_len': 73.373}\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_data, test_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ebb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f25f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47066468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f064b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd90ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
